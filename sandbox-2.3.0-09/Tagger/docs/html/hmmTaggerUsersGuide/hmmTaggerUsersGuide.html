<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>Tagger Annotator Documentation</title><link rel="stylesheet" href="css/stylesheet-html.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.72.0"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="book" lang="en" id="d0e2"><div class="titlepage"><div><div><h1 class="title"><a name="d0e2"></a>Tagger Annotator Documentation</h1></div><div><div class="authorgroup"><h3 class="corpauthor">Authors: The Apache UIMA Development Community</h3></div></div><div><span class="productname">Apache UIMA Sandbox<br></span></div><div><p class="releaseinfo">Version 2.3.0</p></div><div><p class="copyright">Copyright &copy; 2008, 2009 The Apache Software Foundation</p></div><div><div class="legalnotice"><a name="d0e15"></a><p> </p><p><b>Incubation Notice and Disclaimer.&nbsp;</b>Apache UIMA is an effort undergoing incubation at the Apache Software Foundation (ASF). 
          Incubation is required of all newly accepted projects until a further review indicates that 
          the infrastructure, communications, and decision making process have stabilized in a manner 
          consistent with other successful ASF projects. While incubation status is not necessarily 
          a reflection of the completeness or stability of the code, 
          it does indicate that the project has yet to be fully endorsed by the ASF.</p><p> </p><p> </p><p><b>License and Disclaimer.&nbsp;</b>The ASF licenses this documentation
           to you under the Apache License, Version 2.0 (the
           "License"); you may not use this documentation except in compliance
           with the License.  You may obtain a copy of the License at
         
         </p><div class="blockquote"><blockquote class="blockquote"><p>
             <a xmlns:xlink="http://www.w3.org/1999/xlink" href="http://www.apache.org/licenses/LICENSE-2.0" target="_top">http://www.apache.org/licenses/LICENSE-2.0</a>
           </p></blockquote></div><p>
         
           Unless required by applicable law or agreed to in writing,
           this documentation and its contents are distributed under the License 
           on an 
           "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
           KIND, either express or implied.  See the License for the
           specific language governing permissions and limitations
           under the License.
         </p><p> </p><p> </p><p><b>Trademarks.&nbsp;</b>All terms mentioned in the text that are known to be trademarks or 
        service marks have been appropriately capitalized.  Use of such terms
        in this book should not be regarded as affecting the validity of the
        the trademark or service mark.
        </p></div></div></div><hr></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="preface"><a href="#sandbox.tagger.introduction">Introduction</a></span></dt><dt><span class="chapter"><a href="#sandbox.tagger.prerequisites">1. Prerequisites</a></span></dt><dt><span class="chapter"><a href="#sandbox.tagger.processingOverview">2. Processing Overview</a></span></dt><dt><span class="chapter"><a href="#sandbox.tagger.annotatorDescriptor">3. Annotator Descriptor</a></span></dt><dd><dl><dt><span class="section"><a href="#sandbox.tagger.annotatorDescriptor.configParam">3.1. Configuration Parameters</a></span></dt><dt><span class="section"><a href="#sandbox.tagger.annotatorDescriptor.capabilities">3.2. Capabilities</a></span></dt></dl></dd><dt><span class="chapter"><a href="#sandbox.tagger.unittest">4. Functionality Test</a></span></dt><dt><span class="chapter"><a href="#sandbox.tagger.tagger">5. Overview of the Tagger package</a></span></dt><dt><span class="chapter"><a href="#sandbox.tagger.training">6. Training Own Models</a></span></dt><dt><span class="chapter"><a href="#sandbox.tagger.evaluation">7. Evaluation</a></span></dt><dt><span class="appendix"><a href="#sandbox.tagger.theory">A. Theory Behind</a></span></dt><dt><span class="glossary"><a href="#d0e638">Glossary</a></span></dt><dt><span class="bibliography"><a href="#d0e661">Bibliography</a></span></dt></dl></div><div class="preface" lang="en" id="sandbox.tagger.introduction"><div class="titlepage"><div><div><h2 class="title"><a name="sandbox.tagger.introduction"></a>Introduction</h2></div></div></div><p>
			Tagger Annotator is an Apache UIMA statistical analysis
			engine that annotates tokens with corresponding grammatical
			types (parts of speech, or just POS). The tagger is a
			standard hidden Markov model (HMM) tagger.
		</p></div><div class="chapter" lang="en" id="sandbox.tagger.prerequisites"><div class="titlepage"><div><div><h2 class="title"><a name="sandbox.tagger.prerequisites"></a>Chapter&nbsp;1.&nbsp;Prerequisites</h2></div></div></div><p>
			The UIMA HMM Tagger annotator assumes that sentences and
			tokens have already been annotated in the CAS with Sentence
			and Token annotations respectively (see e.g.
			<code class="code">Whitespace Tokenizer Annotator</code>
			).

			Further, the tagger requires a parameter file which
			specifies a number of necessary parameters for tagging
			procedure (see
			<a href="#sandbox.tagger.annotatorDescriptor.configParam" title="3.1.&nbsp;Configuration Parameters">Section&nbsp;3.1, &#8220;Configuration Parameters&#8221;</a>
			).

			Two trained models for English and German are included in
			the package (in the
			<code class="code">resources</code>
			folder). Other models can be trained outside of the UIMA
			framework (see
			<a href="#sandbox.tagger.training" title="Chapter&nbsp;6.&nbsp;Training Own Models">Chapter&nbsp;6, <i xmlns:xlink="http://www.w3.org/1999/xlink">Training Own Models</i></a>
			).
		</p></div><div class="chapter" lang="en" id="sandbox.tagger.processingOverview"><div class="titlepage"><div><div><h2 class="title"><a name="sandbox.tagger.processingOverview"></a>Chapter&nbsp;2.&nbsp;Processing Overview</h2></div></div></div><p>
			The algorithm iterates over sentences and tokens in turn to
			accumulate a list of words. These are then sent to a
			processing engine of HMM tagger. For each
			<code class="code">Token</code>
			, the
			<code class="code">posTag</code>
			field is updated with the corresponding part of speech (e.g.
			<code class="code">posTag = "NN"</code>
			where
			<code class="code">NN</code>
			stands for
			<span class="emphasis"><em>common noun</em></span>
			).
		</p></div><div class="chapter" lang="en" id="sandbox.tagger.annotatorDescriptor"><div class="titlepage"><div><div><h2 class="title"><a name="sandbox.tagger.annotatorDescriptor"></a>Chapter&nbsp;3.&nbsp;Annotator Descriptor</h2></div></div></div><p>
			Two descriptors are employed to configure tagger's
			functionality:
			</p><div class="itemizedlist"><ul type="disc"><li><p>
						<code class="code">HmmTagger.xml</code>
						- is a primitive analysis engine descriptor,
						which defines the tagger basic functionality and
						can be combined in an aggregate analysis engine
						with an arbitrary tokenizer. This descriptor
						cannot be used on itself as the tagger alone
						does not perfom tokenization.
					</p></li><li><p>
						<code class="code">HmmTaggerTAE.xml</code>
						- is an aggregate analysis engine whose only
						function is to combine UIMA
						<code class="code">Whitespace Tokenizer Annotator</code>
						with
						<code class="code">HMM Tagger Annotator</code>
						and is thereby a "ready to use" tagging
						descriptor.
					</p></li></ul></div><p>
		</p><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="sandbox.tagger.annotatorDescriptor.configParam"></a>3.1.&nbsp;Configuration Parameters</h2></div></div></div><p>
				The HMM tagger annotator (
				<code class="code">HmmTagger.xml</code>
				) requires the following configuration parameters:
			</p><p>
				</p><div class="itemizedlist"><ul type="disc"><li><p>
							<code class="code">NGRAM_SIZE</code>
							- this parameter is an Integer, defining
							whether a bi- or trigram model should be
							used for tagging (default is N=3).
							</p><pre class="programlisting"><span class="emphasis"><em>    &lt;configurationParameters&gt;
      &lt;configurationParameter&gt;
        &lt;name&gt;NGRAM_SIZE&lt;/name&gt;
        &lt;type&gt;Integer&lt;/type&gt;
        &lt;multiValued&gt;false&lt;/multiValued&gt;
        &lt;mandatory&gt;true&lt;/mandatory&gt;
      &lt;/configurationParameter&gt;
    &lt;/configurationParameters&gt;
    &lt;configurationParameterSettings&gt;
      &lt;nameValuePair&gt;
        &lt;name&gt;NGRAM_SIZE&lt;/name&gt;
        &lt;value&gt;
          &lt;integer&gt;3&lt;/integer&gt;
        &lt;/value&gt;
      &lt;/nameValuePair&gt;
    &lt;/configurationParameterSettings&gt;</em></span></pre><p>
	</p></li><li><p>
							<code class="code">ModelFile</code>
							- binary file containing the statistical model which should be used for tagging is defined as an external resource
</p><pre class="programlisting"><span class="emphasis"><em>   
    &lt;externalResources&gt;
      &lt;externalResource&gt;
        &lt;name&gt;ModelFile&lt;/name&gt;
        &lt;description&gt;HMM Tagger model file&lt;/description&gt;
        &lt;fileResourceSpecifier&gt;
          &lt;fileUrl&gt;file:german/TuebaModel.dat&lt;/fileUrl&gt;
        &lt;/fileResourceSpecifier&gt;
        &lt;implementationName&gt;org.apache.uima.examples.tagger.ModelResource&lt;/implementationName&gt;
      &lt;/externalResource&gt;
    &lt;/externalResources&gt;</em></span></pre><p>

Thus, one can easily use a different model by changing the <code class="code">fileUrl</code> line: 
<code class="code">file:german/TuebaModel.dat</code>. 
(NB. <span class="emphasis"><em>New models must be located in the <code class="code">resources</code> folder</em></span>.)
After these two parameters have been set, the tagger is ready to use.  
	
							</p></li></ul></div><p>
				  	</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="sandbox.tagger.annotatorDescriptor.capabilities"></a>3.2.&nbsp;Capabilities</h2></div></div></div><p>
				  As the tagger inherits tokenization indexes from the CAS, 
				<code class="code">uima.SentenceAnnotation</code>  and <code class="code">uima.TokenAnnotation</code> with their 
				<code class="code">begin</code> and <code class="code">end</code> features respectively have to be defined as
				input capabilities in the HMM Tagger annotator descriptor. <code class="code">Token</code> receives 
				also an additional <code class="code">posTag</code> feature as an output capability.
				</p><p>
				</p><pre class="programlisting"><span class="emphasis"><em>&lt;capabilities&gt;
      &lt;capability&gt;
        &lt;inputs&gt;
          &lt;type&gt;org.apache.uima.TokenAnnotation&lt;/type&gt;
          &lt;type allAnnotatorFeatures="true"&gt;org.apache.uima.SentenceAnnotation&lt;/type&gt;
          &lt;feature&gt;org.apache.uima.TokenAnnotation:end&lt;/feature&gt;
          &lt;feature&gt;org.apache.uima.TokenAnnotation:begin&lt;/feature&gt;
        &lt;/inputs&gt;
        &lt;outputs&gt;
          &lt;type&gt;org.apache.uima.TokenAnnotation&lt;/type&gt;
          &lt;feature&gt;org.apache.uima.TokenAnnotation:posTag&lt;/feature&gt;
          &lt;feature&gt;org.apache.uima.TokenAnnotation:end&lt;/feature&gt;
          &lt;feature&gt;org.apache.uima.TokenAnnotation:begin&lt;/feature&gt;
        &lt;/outputs&gt;
      &lt;/capability&gt;
    &lt;/capabilities&gt;</em></span></pre><p>
				</p></div></div><div class="chapter" lang="en" id="sandbox.tagger.unittest"><div class="titlepage"><div><div><h2 class="title"><a name="sandbox.tagger.unittest"></a>Chapter&nbsp;4.&nbsp;Functionality Test</h2></div></div></div><p>
			The <code class="code">TaggerTest</code> is a JUnit test file (available in the <code class="code">test</code> folder), 
			which provides an opportunity to test provided models for English and German, 
			as well as the basic functionality of the tagger. In order to check whether 
			the tagger's configuration is correct, just run this file as JUnit Test and you should get the following output:
			
			</p><pre class="programlisting">Tesing German Model... 
The used model is:resources/german/TuebaModel.dat
61646 distinct words in the model
Number of part-of-speech tags used: 54
These are:  [$(, $,, $., ADJA, ADJD, ADV, APPO, APPR, APPRART, APZR, ART, CARD, ... ]
Testing German trigram tagger..
[Jerry, liebt, Wansley, .]
expected: [NE, VVFIN, NE, $.]
tagger output: [NE, VVFIN, NE, $.]
Very Good!
==========================================================
Tesing English Model... 
The used model is:resources/english/BrownModel.dat
56012 distinct words in the model
Number of part-of-speech tags used: 473
These are:  [', '', (, ), *, ,, --, ., :, ``, abl, abn, abx, ap, ap$, at, be, bed, ...]
Testing English trigram tagger...
[Jerry, loves, Wansley, .]
expected: [np, vbz, np, .]
tagger output: [np, vbz, np, .]
Very Good!</pre><p>
			
		</p></div><div class="chapter" lang="en" id="sandbox.tagger.tagger"><div class="titlepage"><div><div><h2 class="title"><a name="sandbox.tagger.tagger"></a>Chapter&nbsp;5.&nbsp;Overview of the Tagger package</h2></div></div></div><p>
			The package <code class="code">org.apache.uima.examples.tagger</code> contains:
			</p><div class="itemizedlist"><ul type="disc"><li><p>
							two interfaces:
							</p><div class="orderedlist"><ol type="1"><li><p>
									<code class="code">IModelResource</code>
									- model resource interface
									 </p></li><li><p>
									 <code class="code">Tagger</code>
									 - general tagger interface, in case one would want to integrate further tagger types. 
									</p></li></ol></div><p>
							
							</p></li><li><p> three classes:
						</p><div class="orderedlist"><ol type="1"><li><p>
									 <code class="code">HMMTagger</code>
									- hidden Markov model tagger for UIMA, that is using Viterbi algorithm to compute the most 
									probable part-of-speech sequence for a given list of tokens.
							</p></li><li><p>
									 <code class="code">Viterbi</code>
									 - implementation of the Viterbi Algorithm. This class makes up the core of the tagger.
									</p></li><li><p>
									 <code class="code">ModelResource.java</code>
									 - implementation of the <code class="code">IModelResource</code>
									</p></li></ol></div><p>
						
						</p></li></ul></div><p>
				</p></div><div class="chapter" lang="en" id="sandbox.tagger.training"><div class="titlepage"><div><div><h2 class="title"><a name="sandbox.tagger.training"></a>Chapter&nbsp;6.&nbsp;Training Own Models</h2></div></div></div><p>
			Though we decide not to include training directly into UIMA framework, one can easily 
			train other models for different pre-annotated corpora outside of the UIMA using <code class="code">ModelGeneration</code> class,
			available in the subpackage <code class="code">org.apache.uima.examples.tagger.trainAndTest</code>.
			This subpackage includes some further files needed for training of own models:
		
			 
			</p><div class="itemizedlist"><ul type="disc"><li><p>
							<code class="code">MappingInterface</code>
									 - defines mapping for a tagset. For example, one may wish to map a more detailed tagset 
									 to a less distinctive one (i.e. tell a program to tag all verbs as just <code class="code">VERB</code>
									 instead of differentiating between <code class="code">verb infinitive</code>, <code class="code">verb imperative</code>, etc.
									
									Two sample implementations for <code class="code">MappingInterface</code> are included, 
									namely <code class="code">TagMappingBrown</code> (mapping reducing Brown corpus tagset from more than 400 tags to 93) and 
									<code class="code">GrobMappingTueba</code>(mapping German STTS tagset from 54 tags to basic 11 categories plus special symbols and punctuation)
 						</p></li><li><p>
							<code class="code">ModelGeneration</code>
							- trains an N-gram model for the tagger, iterating over a List of <code class="code">Token</code>s. 
							Writes the resulting model to a binary file. At the moment,
							only bi-and trigram models are supported. Further N-grams can be easily integrated.
							<code class="code">ModelGeneration</code> is not concerned with the fact, 
							whether the training corpus is given as a single file or as a directory containing a number of files,
							as this is a <code class="code">CORPUS_READER</code> implementation issue. Two supplied readers include both a reader 
							for a corpus as a single file (<code class="code">TT_FormatReader</code>code&gt;) or as a directory (<code class="code">BrownReader</code>code&gt;)
							</p></li><li><p>
							Interface <code class="code">CorpusReader</code>
							- should be used to implement corpus readers for own corpora; the objective 
							of the reader is to take charge of the preprocessing and transform tokenized units 
							(usually <span class="emphasis"><em>words</em></span>) into a List of <code class="code">Token</code> objects. 
							Two sample implementations of <code class="code">CorpusReader</code> are included: 
							
							</p><div class="orderedlist"><ol type="1"><li><p>
									 <code class="code">BrownReader</code>
									-  for the Brown corpus from the nltk distribution (nltk.sourceforge.net) 
									</p></li><li><p>
									 <code class="code">TT_FormatReader</code>
									-  for the corpora in TreeTagger format, i.e. one word per line 
									with tags separated from the words by tabs. 
									</p></li></ol></div><p>
							
							</p></li></ul></div><p>
				</p><p>
		To train a new model, one should  adjust a number of parameters in the <code class="code">"tagger.properties"</code> file, 
		which is in Java properties file format (see <a href="#properties.file">tagger.properties file</a>). After the parameters are set, you just need to run 
		<code class="code">ModelGeneration.java</code>
				<a name="properties.file"></a></p><pre class="programlisting"><span class="emphasis"><em>######## This is the default tagger.properties file
######## This file is used for training and testing only,
######## The configuration for tagging is directly tuned in the descriptor "HmmTagger.xml"


##########################  BOTH FOR TRAINING AND EVALUATION  ################################

######## THESE ARE THE DEFAULT MODEL FILES FOR GERMAN AND ENGLISH
######## You can either uncomment one of them, if you want to replace given models with your own one,

#MODEL_FILE = resources/german/TuebaModel.dat
#MODEL_FILE = resources/english/BrownModel.dat

######## or specify a completely different name
MODEL_FILE = 

######## If mapping of tags is desired, uncomment the following
#DO_MAPPING = true


#######	EXAMPLES OF MAPPING CLASSES 

## Basic mapping for the Brown corpus (nltk distribution) tagset: to get 93 tags out of 473
#MAPPING = org.apache.uima.examples.tagger.TagMappingBrown
## Basic mapping for STTS tagset: from 54 tags onto the basic ca. 15 classes plus punctuation
#MAPPING = org.apache.uima.examples.tagger.GrobMappingTueba

## If you implement your own mapping, you should specify here in the same manner as above a java-path to the class  
MAPPING = 

####### FILE CONTAINING TRAINING CORPUS:
####### can be in specified either as an absolute or as a relative path
####### e.g. FILE = ../../tueba_tigerFormat.txt or FILE = C:/Data/tueba.txt
FILE = 

######## If corpus is in a different format and cannot be read with the provided READERS,
######## you should specify here a java-path to the class (s. examples below) 

#CORPUS_READER = org.apache.uima.examples.tagger.trainAndTest.TT_FormatReader
#CORPUS_READER = org.apache.uima.examples.tagger.trainAndTest.BrownReader
CORPUS_READER = 

#################      ONLY FOR EVALUATION   ###############################

######### GOLD STANDARD CORPUS FILE: 
######### can be specified as an absolute or as a relative path 
## e.g. GOLD_STANDARD = ../../tueba_tigerFormat.txt or GOLD_STANDARD = C:/Data/tueba.txt
GOLD_STANDARD = 

######### Here we specify whether one intends to test a bi- or a trigram model (default is a trigram model)  
N=3
</em></span>
				</pre><p>
			
				</p></div><div class="chapter" lang="en" id="sandbox.tagger.evaluation"><div class="titlepage"><div><div><h2 class="title"><a name="sandbox.tagger.evaluation"></a>Chapter&nbsp;7.&nbsp;Evaluation</h2></div></div></div><p>
		To evaluate performance if a "gold standard" corpus is available, one can use the following provided file:
		</p><div class="itemizedlist"><ul type="disc"><li><p>
					<code class="code">TaggerEvaluation.java</code>
						- can be used to evaluate the tagger and/or new models on a manually annotated corpus. 
				</p></li></ul></div><p>
		</p><p>
		<code class="code">HMMTagger</code> was evaluated for English and German. For English, it was trained on 80% of the Brown corpus 
		(180,000 tokens) and tested on the rest unseen 20%. The achieved accuracy was about 96%, test corpus contained 4.5% of unknown tokens.
		</p><p>
		For German, it achieves between 95% and 96% accuracy when trained and tested on the same type of corpus, i.e. with 80% of corpus used for training and 20% for testing.
		The accuracy goes a bit down when tagging is performed for different types of corpora than the training one, mostly due to the growing number of unknown words.  
		</p></div><div class="appendix" lang="en" id="sandbox.tagger.theory"><div class="titlepage"><div><div><h2 class="title"><a name="sandbox.tagger.theory"></a>Appendix&nbsp;A.&nbsp;Theory Behind</h2></div></div></div><p>
			This chapter is just a sketch of the statistical model
			undelying the tagger.

			Hidden Markov Models (HMMs) are the mainstay of the
			applications employing statistical modeling in any form,
			like speech recognition and production systems, signal
			processing, part of speech tagging.

			A Hidden Markov Model is a probabilistic function of a
			Markov process. A Markov process is a process that fulfills
			Markov assumptions.


			Markov assumptions are:
			</p><div class="itemizedlist"><ul type="disc"><li><p>
						<code class="code">limited horizon</code>
						- Markov processes are states without memory,
						except for condition of the current state.
						Though we usually consider sequences of
						variables that are not independent of each
						other, it often suffices to know the value of
						the current situation without going deep into
						the past happenings. As [
						<a xmlns:xlink="http://www.w3.org/1999/xlink" href="#schuetze">ManningSchuetze99</a>
						] put it, we do not really need to know, how
						many books were in the library last week or last
						year in order to predict how many books there
						will be tomorrow. It is often enough to know the
						current situation. Thereby, future states in the
						Markov process are independent of the past, they
						only depend on the present. Let
						<span class="mathphrase">
								X = (X
								<sub>1</sub>
								, ..., X
								<sub>T</sub>
								)
							</span>
						be a sequence of random variables taking the
						values from the finite state space
						<span class="mathphrase">
								S = (s
								<sub>1</sub>
								, ..., s
								<sub>N</sub>
								)
							</span>
						, then a limited horizon property could be
						formalized by:
						</p><div class="informalequation"><span class="mathphrase">
								P(X
								<sub>t+1</sub>
								= s
								<sub>k</sub>
								|X
								<sub>1</sub>
								, ..., X
								<sub>t</sub>
								) = P(X
								<sub>t+1</sub>
								= s
								<sub>k</sub>
								|X
								<sub>t</sub>
								)
							</span></div><p>


					</p></li><li><p>
						<code class="code">time invariance</code>
					</p><p>
						The probabilities do not change over time, i.e.
						if we know that the probability of observing a
						rainbow after the rain is equal to 90\%, we know
						that it should be true for today as well as for
						tomorrow.
					</p></li></ul></div><p>
		</p><p>

			If
			<code class="code">X</code>
			conforms to these two properties, then it is said to be a
			Markov chain.

			One can describe a Markov chain by a transition matrix:
			</p><div class="informalequation"><span class="mathphrase">
					A = a
					<sub>i,j</sub>
					= P(X
					<sub>t+1</sub>
					= s
					<sub>j</sub>
					|X
					<sub>t</sub>
					=s
					<sub>i</sub>
					)
				</span></div><p>


			</p><div class="informalequation"><span class="mathphrase">
					- with a
					<sub>i,j</sub>
					&gt;= 0 (for all
					<span class="emphasis"><em>i,j</em></span>
					) and the sum of all transition probabilities from
					state
					<span class="emphasis"><em>i</em></span>
					(a
					<sub>i,j</sub>
					) should be equal to 1 (for all
					<span class="emphasis"><em>i</em></span>
					)
				</span></div><p>

		</p><p>

			Markov models can be used whenever one needs to model the
			probability of a linear sequence of variables.

			One distinguishes Visible Markov Models (VMM) vs. Hidden
			Markov Models. The difference is that when we work with
			"visible" events, we can directly estimate the corresponding
			probabilities (which is the case if training corpus is
			available to train own models for HMM taggers).

			Finding a sequence of part of speech tags (i.e. Viterbi part
			of the tagger) in contrast is a hidden Markov model as the
			states (tags) are not directly observable.
		</p><p>
			<span class="emphasis"><em>The goal of HMM - based tagger</em></span>
			is to find part of speech tags ( = hidden states) that
			generate a sequence of words ( = observable states). Most of
			the known implementations of POS taggers are viewing text as
			being produced by a hidden Markov model, so that tagging can
			be regarded as a Markov process deciding which states the
			system went through to generate a given text.

		</p><p>
			<span class="emphasis"><em>General Form of HMM</em></span>
		</p><p>
			A HMM is a five-tuple:
			<span class="mathphrase">(S, O, &#960;, A, B)</span>
			</p><div class="informalexample"><p>where:</p><p>
					</p><div class="itemizedlist"><ul type="disc"><li><p>
								<code class="code">S</code>
								- the set of states (here: parts of
								speech)
							</p></li><li><p>
								<code class="code">K</code>
								- the set of observations (here: words)
							</p></li><li><p>
								<code class="code">&#960;</code>
								- initial state probabilities
							</p></li><li><p>
								<code class="code">A</code>
								- state transitions probabilities
							</p></li><li><p>
								<code class="code">B</code>
								- symbol emissions probabilities
							</p></li></ul></div><p>
				</p></div><p>
		</p><p>
		Further,
		<code class="code">
			X
			<sub>t</sub>
		</code>
		(state sequence) and
		<code class="code">
			O
			<sub>t</sub>
		</code>
		(output sequence) are given.

		Tagging procedure is then the following:
		</p><div class="informalexample"><div class="orderedlist"><ol type="1"><li><p>
						<code class="code">t := 1</code>
					</p></li><li><p>
						<code class="code">
							Start in state s
							<sub>i</sub>
							with probability &#960;
							<sub>i</sub>
							(i.e., X
							<sub>1</sub>
							= i)
						</code>
					</p></li><li><p>
						<code class="code">forever do:</code>
					</p><div class="itemizedlist"><ul type="disc"><li><p>
								<code class="code">
									Move from s
									<sub>i</sub>
									to s
									<sub>j</sub>
									with probability a
									<sub>i,j</sub>
									(i.e. X
									<sub>t+1</sub>
									= j)
								</code>
							</p></li><li><p>
								<code class="code">
									Emit observation symbol o
									<sub>t</sub>
									= k with probability b
									<sub>i,j,k</sub>
								</code>
							</p></li><li><p>
								<code class="code">t := T+1</code>
							</p></li></ul></div></li><li><p>
						<code class="code">end</code>
					</p></li></ol></div></div><p>
</p><p>
			Despite their limitations, HMM-s are one of the most
			successful techniques in natural language processing and are
			widely used, especially in sequence tagging applications.
			The best statistical taggers all perform at about the same
			level of accuracy.
		</p></div><div class="glossary" id="d0e638"><div class="titlepage"><div><div><h2 class="title"><a name="d0e638"></a>Glossary</h2></div></div></div><div class="glossdiv"><h3 class="title">HMM</h3><dl><dt><a name="hmm"></a>Hidden Markov Model</dt><dd><p></p></dd></dl></div><div class="glossdiv"><h3 class="title">POS</h3><dl><dt><a name="pos"></a>Part of Speech</dt><dd><p></p></dd></dl></div></div><div class="bibliography" id="d0e661"><div class="titlepage"><div><div><h2 class="title"><a name="d0e661"></a>Bibliography</h2></div></div></div><div class="biblioentry"><a name="schuetze"></a><p>[ManningSchuetze99] <span class="authorgroup"><span class="firstname">Christopher</span> <span class="surname">Manning</span> and <span class="firstname">Hinrich</span> <span class="surname">Schuetze</span>. </span><span class="title"><i>
				Foundations of Statistical Natural Language Processing
			</i>. </span><span class="copyright">Copyright &copy; 1999. </span><span class="publisher"><span class="publishername">MIT Press. </span></span></p></div></div></div></body></html>